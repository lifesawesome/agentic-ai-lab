{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f23655a",
   "metadata": {},
   "source": [
    "# üçè Health Assistant Evaluation Demo üçé\n",
    "\n",
    "This notebook demonstrates how to use Azure AI Foundry's evaluation capabilities to assess the quality and safety of AI-generated health and fitness responses.\n",
    "\n",
    "## üîê Authentication Setup\n",
    "\n",
    "Before running the next cell, make sure you're authenticated with Azure CLI. Run this command in your terminal:\n",
    "\n",
    "```bash\n",
    "az login --use-device-code\n",
    "```\n",
    "\n",
    "This will provide you with a device code and URL to authenticate in your browser, which is useful for:\n",
    "- Remote development environments\n",
    "- Systems without a default browser\n",
    "- Corporate environments with strict security policies\n",
    "\n",
    "After successful authentication, you can proceed with the notebook cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02495886",
   "metadata": {},
   "source": [
    "## üìä Available Evaluators in Azure AI Foundry\n",
    "\n",
    "Azure AI Foundry provides a comprehensive set of built-in evaluators for different aspects of AI model quality:\n",
    "\n",
    "### **AI Quality (AI Assisted)**\n",
    "- **Groundedness** - Measures how well responses are grounded in provided context\n",
    "- **Relevance** - Evaluates how relevant responses are to the input query  \n",
    "- **Coherence** - Assesses logical flow and consistency in responses\n",
    "- **Fluency** - Measures language quality and readability\n",
    "- **GPT Similarity** - Compares responses to reference answers\n",
    "\n",
    "### **AI Quality (NLP Metrics)**\n",
    "- **F1 Score** - Measures precision and recall balance\n",
    "- **ROUGE Score** - Evaluates text summarization quality\n",
    "- **BLEU Score** - Measures translation and generation quality\n",
    "- **GLEU Score** - Google's BLEU variant for better correlation\n",
    "- **METEOR Score** - Considers synonyms and stemming\n",
    "\n",
    "### **Risk and Safety**\n",
    "- **Violence** - Detects violent content\n",
    "- **Sexual** - Identifies sexual content\n",
    "- **Self-harm** - Detects self-harm related content\n",
    "- **Hate/Unfairness** - Identifies hateful or unfair content\n",
    "- **Protected Material** - Detects copyrighted content\n",
    "- **Indirect Attack** - Identifies indirect prompt injection attempts\n",
    "\n",
    "üìö **For complete details on all available evaluators, their parameters, and usage examples, visit:**  \n",
    "**[Azure AI Foundry Evaluators Documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/observability)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c0ebe",
   "metadata": {},
   "source": [
    "# üèãÔ∏è‚Äç‚ôÄÔ∏è Azure AI Foundry Evaluations üèãÔ∏è‚Äç‚ôÇÔ∏è\n",
    "\n",
    "This notebook demonstrates how to evaluate AI models using Azure AI Foundry with both **local** and **cloud** evaluations.\n",
    "\n",
    "## What This Notebook Does:\n",
    "1. **Setup & Data Creation** - Creates synthetic health & fitness Q&A data\n",
    "2. **Local Evaluation** - Runs F1Score and Relevance evaluators locally  \n",
    "3. **Cloud Evaluation** - Uploads results to Azure AI Foundry project\n",
    "\n",
    "## Key Features:\n",
    "‚úÖ **Local Evaluations** - F1Score and AI-assisted Relevance evaluators\n",
    "‚úÖ **Cloud Integration** - Upload results to Azure AI Foundry\n",
    "‚úÖ **Browser Authentication** - Uses InteractiveBrowserCredential  \n",
    "‚úÖ **Error Handling** - Robust fallbacks and clear status reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b889daf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment variables loaded from: ../../../.env\n",
      "üîç Environment Variables Status:\n",
      "   AI_FOUNDRY_PROJECT_ENDPOINT: ‚úÖ Set\n",
      "   TENANT_ID: ‚úÖ Set\n",
      "\n",
      "‚úÖ All environment variables configured correctly!\n",
      "üîß Loaded values:\n",
      "   AI_FOUNDRY_PROJECT_ENDPOINT: https://demopocaifoundry.services.ai.azure.com/api/projects/demoproject\n",
      "   TENANT_ID: 16b3c013-d300-468d-ac64-7eda0820b6d3\n",
      "‚úÖ Evaluation data created: C:\\src\\ai-foundry-e2e-lab\\observability-and-evaluations\\health_fitness_eval_data.jsonl\n",
      "üìä Total samples: 3\n"
     ]
    }
   ],
   "source": [
    "# Setup and Data Creation\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from root directory\n",
    "root_env_path = os.environ.get(\"ROOT_ENV_PATH\", '../../../.env')\n",
    "load_dotenv(root_env_path)\n",
    "print(f\"‚úÖ Environment variables loaded from: {root_env_path}\")\n",
    "\n",
    "# Check required environment variables for Azure AI Foundry\n",
    "AI_FOUNDRY_PROJECT_ENDPOINT = os.environ.get(\"AI_FOUNDRY_PROJECT_ENDPOINT\")\n",
    "TENANT_ID = os.environ.get(\"TENANT_ID\")\n",
    "\n",
    "print(\"üîç Environment Variables Status:\")\n",
    "print(\n",
    "    f\"   AI_FOUNDRY_PROJECT_ENDPOINT: {'‚úÖ Set' if AI_FOUNDRY_PROJECT_ENDPOINT else '‚ùå Missing'}\"\n",
    ")\n",
    "print(f\"   TENANT_ID: {'‚úÖ Set' if TENANT_ID else '‚ùå Missing'}\")\n",
    "\n",
    "if not AI_FOUNDRY_PROJECT_ENDPOINT:\n",
    "    print(\"\\n‚ö†Ô∏è Required environment variables missing!\")\n",
    "    print(\"Please add these to your .env file:\")\n",
    "    print(\"AI_FOUNDRY_PROJECT_ENDPOINT=<your-azure-ai-project-endpoint>\")\n",
    "    print(\"TENANT_ID=<your-azure-tenant-id>\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All environment variables configured correctly!\")\n",
    "    print(f\"üîß Loaded values:\")\n",
    "    print(f\"   AI_FOUNDRY_PROJECT_ENDPOINT: {AI_FOUNDRY_PROJECT_ENDPOINT}\")\n",
    "    print(f\"   TENANT_ID: {TENANT_ID}\")\n",
    "\n",
    "# Create synthetic health & fitness evaluation data\n",
    "synthetic_eval_data = [\n",
    "    {\n",
    "        \"query\": \"How can I start a beginner workout routine at home?\",\n",
    "        \"context\": \"Workout routines can include push-ups, bodyweight squats, lunges, and planks.\",\n",
    "        \"response\": \"You can just go for 10 push-ups total.\",\n",
    "        \"ground_truth\": \"At home, you can start with short, low-intensity workouts: push-ups, lunges, planks.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Are diet sodas healthy for daily consumption?\",\n",
    "        \"context\": \"Sugar-free or diet drinks may reduce sugar intake, but they still contain artificial sweeteners.\",\n",
    "        \"response\": \"Yes, diet sodas are 100% healthy.\",\n",
    "        \"ground_truth\": \"Diet sodas have fewer sugars than regular soda, but 'healthy' is not guaranteed due to artificial additives.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What's the capital of France?\",\n",
    "        \"context\": \"France is in Europe. Paris is the capital.\",\n",
    "        \"response\": \"London.\",\n",
    "        \"ground_truth\": \"Paris.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Write data to JSONL file\n",
    "eval_data_filename = os.environ.get(\"EVAL_DATA_FILENAME\", \"health_fitness_eval_data.jsonl\")\n",
    "eval_data_path = Path(f\"./{eval_data_filename}\")\n",
    "with eval_data_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for row in synthetic_eval_data:\n",
    "        f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Evaluation data created: {eval_data_path.resolve()}\")\n",
    "print(f\"üìä Total samples: {len(synthetic_eval_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d5598",
   "metadata": {
    "id": "3-Local-Evaluation"
   },
   "source": [
    "## üîç Local Evaluation\n",
    "\n",
    "Run evaluations locally using F1Score (basic text similarity) and Relevance (AI-assisted) evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f04f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Running Local Evaluation...\n",
      "ü§ñ Adding AI-assisted Relevance evaluator...\n",
      "2026-01-02 10:32:48 -0600   22648 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2026-01-02 10:32:48 -0600   22648 execution.bulk     INFO     Average execution time for completed lines: 0.0 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"f1_score_20260102_163248_643561\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2026-01-02 16:32:48.643561+00:00\"\n",
      "Duration: \"0:00:01.013010\"\n",
      "\n",
      "2026-01-02 10:32:54 -0600   45700 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2026-01-02 10:32:54 -0600   45700 execution.bulk     INFO     Average execution time for completed lines: 5.46 seconds. Estimated time for incomplete lines: 10.92 seconds.\n",
      "2026-01-02 10:32:54 -0600   45700 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2026-01-02 10:32:54 -0600   45700 execution.bulk     INFO     Average execution time for completed lines: 2.77 seconds. Estimated time for incomplete lines: 2.77 seconds.\n",
      "2026-01-02 10:32:54 -0600   45700 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2026-01-02 10:32:54 -0600   45700 execution.bulk     INFO     Average execution time for completed lines: 1.92 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"relevance_20260102_163248_636419\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2026-01-02 16:32:48.636419+00:00\"\n",
      "Duration: \"0:00:05.822927\"\n",
      "\n",
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"f1_score\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:01.013010\",\n",
      "        \"completed_lines\": 3,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"relevance\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:05.822927\",\n",
      "        \"completed_lines\": 3,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n",
      "‚úÖ Local evaluation completed!\n",
      "üìä f1_score.f1_score: 0.1870\n",
      "üíæ Results saved to: local_evaluation_results.json\n",
      "üìä f1_score.f1_threshold: 0.5000\n",
      "üíæ Results saved to: local_evaluation_results.json\n",
      "üìä relevance.relevance: 1.6667\n",
      "üíæ Results saved to: local_evaluation_results.json\n",
      "üìä relevance.gpt_relevance: 1.6667\n",
      "üíæ Results saved to: local_evaluation_results.json\n",
      "üìä relevance.relevance_threshold: 3.0000\n",
      "üíæ Results saved to: local_evaluation_results.json\n",
      "üìä f1_score.binary_aggregate: 0.0000\n",
      "üíæ Results saved to: local_evaluation_results.json\n",
      "üìä relevance.binary_aggregate: 0.0000\n",
      "üíæ Results saved to: local_evaluation_results.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚òÅÔ∏è Setting up Cloud Evaluation with Azure AI Foundry...\n",
      "üè¢ Foundry Project Endpoint: https://demopocaifoundry.services.ai.azure.com/api/projects/demoproject\n",
      "üîë Tenant ID: 16b3c013-d300-468d-ac64-7eda0820b6d3\n",
      "üîê Setting up authentication...\n",
      "‚úÖ AIProjectClient created successfully!\n",
      "üì§ Uploading evaluation data to Azure AI Foundry...\n",
      "‚úÖ Data uploaded successfully! Dataset ID: azureai://accounts/demopocaifoundry/projects/demoproject/data/health-fitness-dataset-1767374513/versions/1.0\n",
      "‚öôÔ∏è Configuring evaluators for cloud evaluation...\n",
      "üöÄ Creating and submitting cloud evaluation...\n",
      "üéâ CLOUD EVALUATION SUBMITTED!\n",
      "   üìã Name: b203928d-ee9e-43c1-bd56-8fdf35a39b6f\n",
      "   üìã Status: NotStarted\n",
      "   üìã Response Type: <class 'azure.ai.projects.models._models.Evaluation'>\n",
      "   üìã ID: b203928d-ee9e-43c1-bd56-8fdf35a39b6f\n",
      "\n",
      "üîó View detailed results at: https://ai.azure.com/\n",
      "   Navigate to your project ‚Üí Evaluation ‚Üí View evaluation runs\n",
      "üíæ Results saved to: cloud_evaluation_results.json\n",
      "\n",
      "‚úÖ SUCCESS: Cloud evaluation submitted to Azure AI Foundry!\n",
      "   The evaluation will run in the cloud and results will be available in the Azure AI Foundry portal.\n",
      "üßπ Cleaning up evaluation files...\n",
      "‚úÖ Deleted: health_fitness_eval_data.jsonl\n",
      "‚úÖ Deleted: local_evaluation_results.json\n",
      "‚úÖ Deleted: cloud_evaluation_results.json\n",
      "\n",
      "üéâ Cleanup completed! Deleted 3 file(s).\n",
      "‚òÅÔ∏è Cleaning up cloud resources from Azure AI Foundry...\n",
      "‚ö†Ô∏è  No cloud results file found. You can manually specify the dataset name:\n",
      "‚úÖ Connected to Azure AI Foundry\n",
      "\n",
      "üìä Listing datasets...\n",
      "Found 6 dataset(s):\n",
      "  1. Name: health-fitness-dataset-1767374513, ID: azureai://accounts/demopocaifoundry/projects/demoproject/data/health-fitness-dataset-1767374513/versions/1.0, Version: 1.0\n",
      "  2. Name: assistant-C2XrwTMjXNmtZRR5QfQakR, ID: azureai://accounts/demopocaifoundry/projects/demoproject/data/assistant-C2XrwTMjXNmtZRR5QfQakR/versions/1, Version: 1\n",
      "  3. Name: assistant-PUSEGHvRiPpAVWBmoszzdT, ID: azureai://accounts/demopocaifoundry/projects/demoproject/data/assistant-PUSEGHvRiPpAVWBmoszzdT/versions/1, Version: 1\n",
      "  4. Name: assistant-FJQK6R8ztKv2DGpfTWSWX5, ID: azureai://accounts/demopocaifoundry/projects/demoproject/data/assistant-FJQK6R8ztKv2DGpfTWSWX5/versions/1, Version: 1\n",
      "  5. Name: assistant-57yNrVqQyiyeG6GKqbTqjz, ID: azureai://accounts/demopocaifoundry/projects/demoproject/data/assistant-57yNrVqQyiyeG6GKqbTqjz/versions/1, Version: 1\n",
      "  6. Name: assistant-HPBw1qksbbpoBeTbPMYaiR, ID: azureai://accounts/demopocaifoundry/projects/demoproject/data/assistant-HPBw1qksbbpoBeTbPMYaiR/versions/1, Version: 1\n",
      "‚è≠Ô∏è  Skipped dataset deletion\n",
      "\n",
      "üéâ Cloud cleanup completed!\n"
     ]
    }
   ],
   "source": [
    "# Local Evaluation with Azure AI Foundry\n",
    "from azure.ai.evaluation import evaluate, F1ScoreEvaluator, RelevanceEvaluator\n",
    "import logging\n",
    "\n",
    "# Reduce logging noise\n",
    "logging.getLogger('promptflow').setLevel(logging.ERROR)\n",
    "logging.getLogger('azure.ai.evaluation').setLevel(logging.WARNING)\n",
    "\n",
    "print(\"üîç Running Local Evaluation...\")\n",
    "\n",
    "# Configure evaluators\n",
    "evaluators = {\n",
    "    \"f1_score\": F1ScoreEvaluator()\n",
    "}\n",
    "\n",
    "evaluator_config = {\n",
    "    \"f1_score\": {\n",
    "        \"column_mapping\": {\n",
    "            \"response\": \"${data.response}\",\n",
    "            \"ground_truth\": \"${data.ground_truth}\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add AI-assisted evaluator if Azure OpenAI is configured\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\", \"\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\", os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4\")),\n",
    "    \"api_version\": os.environ.get(\"AOAI_API_VERSION\", os.environ.get(\"API_VERSION\", \"2024-02-15-preview\")),\n",
    "}\n",
    "\n",
    "if model_config[\"azure_endpoint\"] and model_config[\"api_key\"]:\n",
    "    print(\"ü§ñ Adding AI-assisted Relevance evaluator...\")\n",
    "    evaluators[\"relevance\"] = RelevanceEvaluator(model_config=model_config)\n",
    "    evaluator_config[\"relevance\"] = {\n",
    "        \"column_mapping\": {\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\"\n",
    "        }\n",
    "    }\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Azure OpenAI not configured - using F1Score only\")\n",
    "\n",
    "# Run local evaluation\n",
    "try:\n",
    "    local_result = evaluate(\n",
    "        data=str(eval_data_path),\n",
    "        evaluators=evaluators,\n",
    "        evaluator_config=evaluator_config\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Local evaluation completed!\")\n",
    "    \n",
    "    # Display results\n",
    "    metrics = local_result['metrics']\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"üìä {metric_name}: {value:.4f}\")\n",
    "        \n",
    "        # Save results locally\n",
    "        local_results_filename = os.environ.get(\"LOCAL_RESULTS_FILENAME\", \"local_evaluation_results.json\")\n",
    "        with open(local_results_filename, \"w\") as f:\n",
    "            json.dump(local_result, f, indent=2)\n",
    "\n",
    "        print(f\"üíæ Results saved to: {local_results_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Local evaluation failed: {e}\")\n",
    "    local_result = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d525400",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è Cloud Evaluation\n",
    "\n",
    "Upload evaluation results to Azure AI Foundry project for tracking and collaboration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "965f796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Evaluation - Following Official Microsoft Documentation\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import (\n",
    "    EvaluatorConfiguration,\n",
    "    EvaluatorIds,\n",
    "    Evaluation,\n",
    "    InputDataset\n",
    ")\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "print(\"‚òÅÔ∏è Setting up Cloud Evaluation with Azure AI Foundry...\")\n",
    "\n",
    "# Configuration from environment variables\n",
    "AI_FOUNDRY_PROJECT_ENDPOINT = os.environ.get(\"AI_FOUNDRY_PROJECT_ENDPOINT\")\n",
    "TENANT_ID = os.environ.get(\"TENANT_ID\")\n",
    "\n",
    "print(f\"üè¢ Foundry Project Endpoint: {AI_FOUNDRY_PROJECT_ENDPOINT}\")\n",
    "print(f\"üîë Tenant ID: {TENANT_ID}\")\n",
    "\n",
    "if not AI_FOUNDRY_PROJECT_ENDPOINT:\n",
    "    print(\"‚ö†Ô∏è Missing AI_FOUNDRY_PROJECT_ENDPOINT in .env file\")\n",
    "    cloud_result = None\n",
    "else:\n",
    "    try:\n",
    "        # Step 1: Create project client using DefaultAzureCredential (as per Microsoft docs)\n",
    "        print(\"üîê Setting up authentication...\")\n",
    "        project_client = AIProjectClient(\n",
    "            endpoint=AI_FOUNDRY_PROJECT_ENDPOINT,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        print(\"‚úÖ AIProjectClient created successfully!\")\n",
    "\n",
    "        # Step 2: Upload evaluation data to Azure AI Foundry (required for cloud evaluation)\n",
    "        print(\"üì§ Uploading evaluation data to Azure AI Foundry...\")\n",
    "        dataset_name = os.environ.get(\"DATASET_NAME\", f\"health-fitness-dataset-{int(time.time())}\")\n",
    "        dataset_version = os.environ.get(\"DATASET_VERSION\", \"1.0\")\n",
    "        try:\n",
    "            data_upload = project_client.datasets.upload_file(\n",
    "                name=dataset_name,\n",
    "                version=dataset_version,\n",
    "                file_path=str(eval_data_path),\n",
    "            )\n",
    "            data_id = data_upload.id\n",
    "            print(f\"‚úÖ Data uploaded successfully! Dataset ID: {data_id}\")\n",
    "        except Exception as upload_error:\n",
    "            print(f\"‚ùå Data upload failed: {upload_error}\")\n",
    "            raise upload_error\n",
    "\n",
    "        # Step 3: Configure evaluators using Azure AI Foundry built-in evaluators\n",
    "        print(\"‚öôÔ∏è Configuring evaluators for cloud evaluation...\")\n",
    "\n",
    "        evaluators = {\n",
    "            \"bleu_score\": EvaluatorConfiguration(\n",
    "                id=EvaluatorIds.BLEU_SCORE.value,\n",
    "                data_mapping={\n",
    "                    \"response\": \"${data.response}\",\n",
    "                    \"ground_truth\": \"${data.ground_truth}\",\n",
    "                },\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # Step 4: Create and submit evaluation\n",
    "        print(\"üöÄ Creating and submitting cloud evaluation...\")\n",
    "        evaluation_name = os.environ.get(\"EVALUATION_NAME\", f\"health-fitness-eval-{int(time.time())}\")\n",
    "        evaluation = Evaluation(\n",
    "            display_name=evaluation_name,\n",
    "            description=\"Health and fitness AI response evaluation\",\n",
    "            data=InputDataset(id=data_id),\n",
    "            evaluators=evaluators,\n",
    "        )\n",
    "\n",
    "        # Submit the evaluation\n",
    "        evaluation_response = project_client.evaluations.create(evaluation)\n",
    "\n",
    "        print(\"üéâ CLOUD EVALUATION SUBMITTED!\")\n",
    "        print(f\"   üìã Name: {evaluation_response.name}\")\n",
    "        print(f\"   üìã Status: {evaluation_response.status}\")\n",
    "        print(f\"   üìã Response Type: {type(evaluation_response)}\")\n",
    "\n",
    "        # Get evaluation ID - handle different possible attribute names\n",
    "        evaluation_id = None\n",
    "        if hasattr(evaluation_response, 'id'):\n",
    "            evaluation_id = evaluation_response.id\n",
    "        elif hasattr(evaluation_response, 'name'):\n",
    "            evaluation_id = evaluation_response.name  # Use name as ID if no separate ID exists\n",
    "\n",
    "        if evaluation_id:\n",
    "            print(f\"   üìã ID: {evaluation_id}\")\n",
    "\n",
    "        print(f\"\\nüîó View detailed results at: https://ai.azure.com/\")\n",
    "        print(\"   Navigate to your project ‚Üí Evaluation ‚Üí View evaluation runs\")\n",
    "\n",
    "        # Save results\n",
    "        cloud_result = {\n",
    "            \"evaluation_name\": evaluation_response.name,\n",
    "            \"status\": evaluation_response.status,\n",
    "            \"project_endpoint\": AI_FOUNDRY_PROJECT_ENDPOINT,\n",
    "            \"dataset_id\": data_id,\n",
    "            \"timestamp\": int(time.time()),\n",
    "        }\n",
    "\n",
    "        # Add evaluation ID if available\n",
    "        if evaluation_id:\n",
    "            cloud_result[\"evaluation_id\"] = evaluation_id\n",
    "\n",
    "        with open(os.environ.get(\"CLOUD_RESULTS_FILENAME\", \"cloud_evaluation_results.json\"), \"w\") as f:\n",
    "            json.dump(cloud_result, f, indent=2, default=str)\n",
    "        print(f\"üíæ Results saved to: {os.environ.get('CLOUD_RESULTS_FILENAME', 'cloud_evaluation_results.json')}\")\n",
    "\n",
    "        print(\"\\n‚úÖ SUCCESS: Cloud evaluation submitted to Azure AI Foundry!\")\n",
    "        print(\"   The evaluation will run in the cloud and results will be available in the Azure AI Foundry portal.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Cloud evaluation failed: {e}\")\n",
    "        print(f\"üìã Error type: {type(e).__name__}\")\n",
    "\n",
    "        # Enhanced error handling\n",
    "        error_str = str(e).lower()\n",
    "        if \"401\" in error_str or \"unauthorized\" in error_str:\n",
    "            print(\"\\nüîê AUTHENTICATION ISSUE:\")\n",
    "            print(\"   - Make sure you're logged in with: az login\")\n",
    "            print(\"   - Ensure you have access to the Azure AI Foundry project\")\n",
    "        elif \"403\" in error_str or \"forbidden\" in error_str:\n",
    "            print(\"\\nüö´ PERMISSION ISSUE:\")\n",
    "            print(\"   - Verify you have 'AI Developer' or 'Contributor' role\")\n",
    "            print(\"   - Check Azure AI Foundry project permissions\")\n",
    "        elif \"404\" in error_str or \"not found\" in error_str:\n",
    "            print(\"\\nüîç RESOURCE NOT FOUND:\")\n",
    "            print(\"   - Verify AI_FOUNDRY_PROJECT_ENDPOINT is correct\")\n",
    "            print(\"   - Check if project exists in Azure AI Foundry\")\n",
    "        elif \"storage\" in error_str or \"blob\" in error_str:\n",
    "            print(\"\\nüíæ STORAGE ISSUE:\")\n",
    "            print(\"   - Ensure your Azure AI Foundry project has a connected storage account\")\n",
    "            print(\"   - Check storage account permissions for the project\")\n",
    "        else:\n",
    "            print(f\"\\nüí° TROUBLESHOOTING:\")\n",
    "            print(f\"   - Full error: {str(e)[:300]}...\")\n",
    "            print(\"   - Try running local evaluation first\")\n",
    "            print(\"   - Check Azure AI Foundry project configuration\")\n",
    "\n",
    "        cloud_result = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3035d7d9",
   "metadata": {},
   "source": [
    "## üßπ Cleanup Evaluation Files\n",
    "\n",
    "Remove all generated evaluation data and results files to keep the workspace clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d9fac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup evaluation files\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üßπ Cleaning up evaluation files...\")\n",
    "\n",
    "files_to_cleanup = [\n",
    "    os.environ.get(\"EVAL_DATA_FILENAME\", \"health_fitness_eval_data.jsonl\"),\n",
    "    os.environ.get(\"LOCAL_RESULTS_FILENAME\", \"local_evaluation_results.json\"),\n",
    "    os.environ.get(\"CLOUD_RESULTS_FILENAME\", \"cloud_evaluation_results.json\")\n",
    "]\n",
    "\n",
    "deleted_count = 0\n",
    "for file_path in files_to_cleanup:\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(f\"‚úÖ Deleted: {file_path}\")\n",
    "            deleted_count += 1\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  File not found (already deleted?): {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error deleting {file_path}: {e}\")\n",
    "\n",
    "if deleted_count > 0:\n",
    "    print(f\"\\nüéâ Cleanup completed! Deleted {deleted_count} file(s).\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No files to cleanup (all already deleted).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5abcaf",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è Cleanup Cloud Resources (Azure AI Foundry)\n",
    "\n",
    "Remove uploaded datasets and evaluation results from Azure AI Foundry to keep your cloud workspace clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee59e449",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Cleanup cloud resources from Azure AI Foundry\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "import json\n",
    "\n",
    "print(\"‚òÅÔ∏è Cleaning up cloud resources from Azure AI Foundry...\")\n",
    "\n",
    "try:\n",
    "    # Load saved cloud results to get dataset ID\n",
    "    cloud_results_file = os.environ.get(\"CLOUD_RESULTS_FILENAME\", \"cloud_evaluation_results.json\")\n",
    "    \n",
    "    if os.path.exists(cloud_results_file):\n",
    "        with open(cloud_results_file, \"r\") as f:\n",
    "            cloud_result = json.load(f)\n",
    "            dataset_id = cloud_result.get(\"dataset_id\")\n",
    "            dataset_name = cloud_result.get(\"evaluation_name\", \"\").replace(\"eval\", \"dataset\")\n",
    "            print(f\"üìã Found dataset ID from previous run: {dataset_id}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No cloud results file found. You can manually specify the dataset name:\")\n",
    "        dataset_name = input(\"Enter dataset name (or press Enter to skip): \").strip()\n",
    "        dataset_id = None\n",
    "    \n",
    "    # Connect to Azure AI Foundry\n",
    "    AI_FOUNDRY_PROJECT_ENDPOINT = os.environ.get(\"AI_FOUNDRY_PROJECT_ENDPOINT\")\n",
    "    \n",
    "    if AI_FOUNDRY_PROJECT_ENDPOINT:\n",
    "        project_client = AIProjectClient(\n",
    "            endpoint=AI_FOUNDRY_PROJECT_ENDPOINT,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        print(\"‚úÖ Connected to Azure AI Foundry\")\n",
    "        \n",
    "        # List and delete datasets\n",
    "        print(\"\\nüìä Listing datasets...\")\n",
    "        try:\n",
    "            datasets = project_client.datasets.list()\n",
    "            dataset_list = list(datasets)\n",
    "            \n",
    "            if dataset_list:\n",
    "                print(f\"Found {len(dataset_list)} dataset(s):\")\n",
    "                for idx, ds in enumerate(dataset_list, 1):\n",
    "                    print(f\"  {idx}. Name: {ds.name}, ID: {ds.id}, Version: {ds.version if hasattr(ds, 'version') else 'N/A'}\")\n",
    "                \n",
    "                # Option to delete specific dataset or all\n",
    "                delete_choice = input(\"\\nEnter dataset number to delete (or 'all' to delete all, or 'skip' to skip): \").strip().lower()\n",
    "                \n",
    "                if delete_choice == \"all\":\n",
    "                    for ds in dataset_list:\n",
    "                        try:\n",
    "                            project_client.datasets.delete(name=ds.name, version=ds.version if hasattr(ds, 'version') else None)\n",
    "                            print(f\"‚úÖ Deleted dataset: {ds.name}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"‚ùå Failed to delete {ds.name}: {e}\")\n",
    "                elif delete_choice.isdigit() and 1 <= int(delete_choice) <= len(dataset_list):\n",
    "                    ds = dataset_list[int(delete_choice) - 1]\n",
    "                    try:\n",
    "                        project_client.datasets.delete(name=ds.name, version=ds.version if hasattr(ds, 'version') else None)\n",
    "                        print(f\"‚úÖ Deleted dataset: {ds.name}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ùå Failed to delete dataset: {e}\")\n",
    "                else:\n",
    "                    print(\"‚è≠Ô∏è  Skipped dataset deletion\")\n",
    "            else:\n",
    "                print(\"‚úÖ No datasets found in Azure AI Foundry\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error listing/deleting datasets: {e}\")\n",
    "            print(\"üí° You may need to delete datasets manually in Azure AI Foundry portal:\")\n",
    "            print(f\"   https://ai.azure.com/ ‚Üí Your Project ‚Üí Data ‚Üí Datasets\")\n",
    "        \n",
    "        print(\"\\nüéâ Cloud cleanup completed!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå AI_FOUNDRY_PROJECT_ENDPOINT not set - cannot connect to Azure AI Foundry\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Cloud cleanup failed: {e}\")\n",
    "    print(\"\\nüí° Manual cleanup options:\")\n",
    "    print(\"1. Go to: https://ai.azure.com/\")\n",
    "    print(\"2. Navigate to your project ‚Üí Data ‚Üí Datasets\")\n",
    "    print(\"3. Select and delete the evaluation datasets\")\n",
    "    print(\"4. Navigate to Evaluation ‚Üí View runs to delete evaluation results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
