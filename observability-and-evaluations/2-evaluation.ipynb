{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f23655a",
   "metadata": {},
   "source": [
    "# ğŸ Health Assistant Evaluation Demo ğŸ\n",
    "\n",
    "This notebook demonstrates how to use Azure AI Foundry's evaluation capabilities to assess the quality and safety of AI-generated health and fitness responses.\n",
    "\n",
    "## ğŸ” Authentication Setup\n",
    "\n",
    "Before running the next cell, make sure you're authenticated with Azure CLI. Run this command in your terminal:\n",
    "\n",
    "```bash\n",
    "az login --use-device-code\n",
    "```\n",
    "\n",
    "This will provide you with a device code and URL to authenticate in your browser, which is useful for:\n",
    "- Remote development environments\n",
    "- Systems without a default browser\n",
    "- Corporate environments with strict security policies\n",
    "\n",
    "After successful authentication, you can proceed with the notebook cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02495886",
   "metadata": {},
   "source": [
    "## ğŸ“Š Available Evaluators in Azure AI Foundry\n",
    "\n",
    "Azure AI Foundry provides a comprehensive set of built-in evaluators for different aspects of AI model quality:\n",
    "\n",
    "### **AI Quality (AI Assisted)**\n",
    "- **Groundedness** - Measures how well responses are grounded in provided context\n",
    "- **Relevance** - Evaluates how relevant responses are to the input query  \n",
    "- **Coherence** - Assesses logical flow and consistency in responses\n",
    "- **Fluency** - Measures language quality and readability\n",
    "- **GPT Similarity** - Compares responses to reference answers\n",
    "\n",
    "### **AI Quality (NLP Metrics)**\n",
    "- **F1 Score** - Measures precision and recall balance\n",
    "- **ROUGE Score** - Evaluates text summarization quality\n",
    "- **BLEU Score** - Measures translation and generation quality\n",
    "- **GLEU Score** - Google's BLEU variant for better correlation\n",
    "- **METEOR Score** - Considers synonyms and stemming\n",
    "\n",
    "### **Risk and Safety**\n",
    "- **Violence** - Detects violent content\n",
    "- **Sexual** - Identifies sexual content\n",
    "- **Self-harm** - Detects self-harm related content\n",
    "- **Hate/Unfairness** - Identifies hateful or unfair content\n",
    "- **Protected Material** - Detects copyrighted content\n",
    "- **Indirect Attack** - Identifies indirect prompt injection attempts\n",
    "\n",
    "ğŸ“š **For complete details on all available evaluators, their parameters, and usage examples, visit:**  \n",
    "**[Azure AI Foundry Evaluators Documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/observability)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c0ebe",
   "metadata": {},
   "source": [
    "# ğŸ‹ï¸â€â™€ï¸ Azure AI Foundry Evaluations ğŸ‹ï¸â€â™‚ï¸\n",
    "\n",
    "This notebook demonstrates how to evaluate AI models using Azure AI Foundry with both **local** and **cloud** evaluations.\n",
    "\n",
    "## What This Notebook Does:\n",
    "1. **Setup & Data Creation** - Creates synthetic health & fitness Q&A data\n",
    "2. **Local Evaluation** - Runs F1Score and Relevance evaluators locally  \n",
    "3. **Cloud Evaluation** - Uploads results to Azure AI Foundry project\n",
    "\n",
    "## Key Features:\n",
    "âœ… **Local Evaluations** - F1Score and AI-assisted Relevance evaluators\n",
    "âœ… **Cloud Integration** - Upload results to Azure AI Foundry\n",
    "âœ… **Browser Authentication** - Uses InteractiveBrowserCredential  \n",
    "âœ… **Error Handling** - Robust fallbacks and clear status reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b889daf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment variables loaded from: ../../../.env\n",
      "ğŸ” Environment Variables Status:\n",
      "   AI_FOUNDRY_PROJECT_ENDPOINT: âœ… Set\n",
      "   TENANT_ID: âœ… Set\n",
      "\n",
      "âœ… All environment variables configured correctly!\n",
      "ğŸ”§ Loaded values:\n",
      "   AI_FOUNDRY_PROJECT_ENDPOINT: https://demopocaifoundry.services.ai.azure.com/api/projects/demoproject\n",
      "   TENANT_ID: 16b3c013-d300-468d-ac64-7eda0820b6d3\n",
      "âœ… Evaluation data created: C:\\src\\ai-foundry-e2e-lab\\observability-and-evaluations\\health_fitness_eval_data.jsonl\n",
      "ğŸ“Š Total samples: 3\n"
     ]
    }
   ],
   "source": [
    "# Setup and Data Creation\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from root directory\n",
    "root_env_path = os.environ.get(\"ROOT_ENV_PATH\", '../../../.env')\n",
    "load_dotenv(root_env_path)\n",
    "print(f\"âœ… Environment variables loaded from: {root_env_path}\")\n",
    "\n",
    "# Check required environment variables for Azure AI Foundry\n",
    "AI_FOUNDRY_PROJECT_ENDPOINT = os.environ.get(\"AI_FOUNDRY_PROJECT_ENDPOINT\")\n",
    "TENANT_ID = os.environ.get(\"TENANT_ID\")\n",
    "\n",
    "print(\"ğŸ” Environment Variables Status:\")\n",
    "print(\n",
    "    f\"   AI_FOUNDRY_PROJECT_ENDPOINT: {'âœ… Set' if AI_FOUNDRY_PROJECT_ENDPOINT else 'âŒ Missing'}\"\n",
    ")\n",
    "print(f\"   TENANT_ID: {'âœ… Set' if TENANT_ID else 'âŒ Missing'}\")\n",
    "\n",
    "if not AI_FOUNDRY_PROJECT_ENDPOINT:\n",
    "    print(\"\\nâš ï¸ Required environment variables missing!\")\n",
    "    print(\"Please add these to your .env file:\")\n",
    "    print(\"AI_FOUNDRY_PROJECT_ENDPOINT=<your-azure-ai-project-endpoint>\")\n",
    "    print(\"TENANT_ID=<your-azure-tenant-id>\")\n",
    "else:\n",
    "    print(f\"\\nâœ… All environment variables configured correctly!\")\n",
    "    print(f\"ğŸ”§ Loaded values:\")\n",
    "    print(f\"   AI_FOUNDRY_PROJECT_ENDPOINT: {AI_FOUNDRY_PROJECT_ENDPOINT}\")\n",
    "    print(f\"   TENANT_ID: {TENANT_ID}\")\n",
    "\n",
    "# Create synthetic health & fitness evaluation data\n",
    "synthetic_eval_data = [\n",
    "    {\n",
    "        \"query\": \"How can I start a beginner workout routine at home?\",\n",
    "        \"context\": \"Workout routines can include push-ups, bodyweight squats, lunges, and planks.\",\n",
    "        \"response\": \"You can just go for 10 push-ups total.\",\n",
    "        \"ground_truth\": \"At home, you can start with short, low-intensity workouts: push-ups, lunges, planks.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Are diet sodas healthy for daily consumption?\",\n",
    "        \"context\": \"Sugar-free or diet drinks may reduce sugar intake, but they still contain artificial sweeteners.\",\n",
    "        \"response\": \"Yes, diet sodas are 100% healthy.\",\n",
    "        \"ground_truth\": \"Diet sodas have fewer sugars than regular soda, but 'healthy' is not guaranteed due to artificial additives.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What's the capital of France?\",\n",
    "        \"context\": \"France is in Europe. Paris is the capital.\",\n",
    "        \"response\": \"London.\",\n",
    "        \"ground_truth\": \"Paris.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Write data to JSONL file\n",
    "eval_data_filename = os.environ.get(\"EVAL_DATA_FILENAME\", \"health_fitness_eval_data.jsonl\")\n",
    "eval_data_path = Path(f\"./{eval_data_filename}\")\n",
    "with eval_data_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for row in synthetic_eval_data:\n",
    "        f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "print(f\"âœ… Evaluation data created: {eval_data_path.resolve()}\")\n",
    "print(f\"ğŸ“Š Total samples: {len(synthetic_eval_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d5598",
   "metadata": {
    "id": "3-Local-Evaluation"
   },
   "source": [
    "## ğŸ” Local Evaluation\n",
    "\n",
    "Run evaluations locally using F1Score (basic text similarity) and Relevance (AI-assisted) evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f04f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Running Local Evaluation...\n",
      "ğŸ¤– Adding AI-assisted Relevance evaluator...\n",
      "2026-01-02 10:32:48 -0600   22648 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2026-01-02 10:32:48 -0600   22648 execution.bulk     INFO     Average execution time for completed lines: 0.0 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"f1_score_20260102_163248_643561\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2026-01-02 16:32:48.643561+00:00\"\n",
      "Duration: \"0:00:01.013010\"\n",
      "\n",
      "2026-01-02 10:32:54 -0600   45700 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2026-01-02 10:32:54 -0600   45700 execution.bulk     INFO     Average execution time for completed lines: 5.46 seconds. Estimated time for incomplete lines: 10.92 seconds.\n",
      "2026-01-02 10:32:54 -0600   45700 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2026-01-02 10:32:54 -0600   45700 execution.bulk     INFO     Average execution time for completed lines: 2.77 seconds. Estimated time for incomplete lines: 2.77 seconds.\n",
      "2026-01-02 10:32:54 -0600   45700 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2026-01-02 10:32:54 -0600   45700 execution.bulk     INFO     Average execution time for completed lines: 1.92 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"relevance_20260102_163248_636419\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2026-01-02 16:32:48.636419+00:00\"\n",
      "Duration: \"0:00:05.822927\"\n",
      "\n",
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"f1_score\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:01.013010\",\n",
      "        \"completed_lines\": 3,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"relevance\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:05.822927\",\n",
      "        \"completed_lines\": 3,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n",
      "âœ… Local evaluation completed!\n",
      "ğŸ“Š f1_score.f1_score: 0.1870\n",
      "ğŸ’¾ Results saved to: local_evaluation_results.json\n",
      "ğŸ“Š f1_score.f1_threshold: 0.5000\n",
      "ğŸ’¾ Results saved to: local_evaluation_results.json\n",
      "ğŸ“Š relevance.relevance: 1.6667\n",
      "ğŸ’¾ Results saved to: local_evaluation_results.json\n",
      "ğŸ“Š relevance.gpt_relevance: 1.6667\n",
      "ğŸ’¾ Results saved to: local_evaluation_results.json\n",
      "ğŸ“Š relevance.relevance_threshold: 3.0000\n",
      "ğŸ’¾ Results saved to: local_evaluation_results.json\n",
      "ğŸ“Š f1_score.binary_aggregate: 0.0000\n",
      "ğŸ’¾ Results saved to: local_evaluation_results.json\n",
      "ğŸ“Š relevance.binary_aggregate: 0.0000\n",
      "ğŸ’¾ Results saved to: local_evaluation_results.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â˜ï¸ Setting up Cloud Evaluation with Azure AI Foundry...\n",
      "ğŸ¢ Foundry Project Endpoint: https://demopocaifoundry.services.ai.azure.com/api/projects/demoproject\n",
      "ğŸ”‘ Tenant ID: 16b3c013-d300-468d-ac64-7eda0820b6d3\n",
      "ğŸ” Setting up authentication...\n",
      "âœ… AIProjectClient created successfully!\n",
      "ğŸ“¤ Uploading evaluation data to Azure AI Foundry...\n",
      "âœ… Data uploaded successfully! Dataset ID: azureai://accounts/demopocaifoundry/projects/demoproject/data/health-fitness-dataset-1767374513/versions/1.0\n",
      "âš™ï¸ Configuring evaluators for cloud evaluation...\n",
      "ğŸš€ Creating and submitting cloud evaluation...\n",
      "ğŸ‰ CLOUD EVALUATION SUBMITTED!\n",
      "   ğŸ“‹ Name: b203928d-ee9e-43c1-bd56-8fdf35a39b6f\n",
      "   ğŸ“‹ Status: NotStarted\n",
      "   ğŸ“‹ Response Type: <class 'azure.ai.projects.models._models.Evaluation'>\n",
      "   ğŸ“‹ ID: b203928d-ee9e-43c1-bd56-8fdf35a39b6f\n",
      "\n",
      "ğŸ”— View detailed results at: https://ai.azure.com/\n",
      "   Navigate to your project â†’ Evaluation â†’ View evaluation runs\n",
      "ğŸ’¾ Results saved to: cloud_evaluation_results.json\n",
      "\n",
      "âœ… SUCCESS: Cloud evaluation submitted to Azure AI Foundry!\n",
      "   The evaluation will run in the cloud and results will be available in the Azure AI Foundry portal.\n",
      "ğŸ§¹ Cleaning up evaluation files...\n",
      "âœ… Deleted: health_fitness_eval_data.jsonl\n",
      "âœ… Deleted: local_evaluation_results.json\n",
      "âœ… Deleted: cloud_evaluation_results.json\n",
      "\n",
      "ğŸ‰ Cleanup completed! Deleted 3 file(s).\n"
     ]
    }
   ],
   "source": [
    "# Local Evaluation with Azure AI Foundry\n",
    "from azure.ai.evaluation import evaluate, F1ScoreEvaluator, RelevanceEvaluator\n",
    "import logging\n",
    "\n",
    "# Reduce logging noise\n",
    "logging.getLogger('promptflow').setLevel(logging.ERROR)\n",
    "logging.getLogger('azure.ai.evaluation').setLevel(logging.WARNING)\n",
    "\n",
    "print(\"ğŸ” Running Local Evaluation...\")\n",
    "\n",
    "# Configure evaluators\n",
    "evaluators = {\n",
    "    \"f1_score\": F1ScoreEvaluator()\n",
    "}\n",
    "\n",
    "evaluator_config = {\n",
    "    \"f1_score\": {\n",
    "        \"column_mapping\": {\n",
    "            \"response\": \"${data.response}\",\n",
    "            \"ground_truth\": \"${data.ground_truth}\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add AI-assisted evaluator if Azure OpenAI is configured\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\", \"\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\", os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4\")),\n",
    "    \"api_version\": os.environ.get(\"AOAI_API_VERSION\", os.environ.get(\"API_VERSION\", \"2024-02-15-preview\")),\n",
    "}\n",
    "\n",
    "if model_config[\"azure_endpoint\"] and model_config[\"api_key\"]:\n",
    "    print(\"ğŸ¤– Adding AI-assisted Relevance evaluator...\")\n",
    "    evaluators[\"relevance\"] = RelevanceEvaluator(model_config=model_config)\n",
    "    evaluator_config[\"relevance\"] = {\n",
    "        \"column_mapping\": {\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\"\n",
    "        }\n",
    "    }\n",
    "else:\n",
    "    print(\"âš ï¸ Azure OpenAI not configured - using F1Score only\")\n",
    "\n",
    "# Run local evaluation\n",
    "try:\n",
    "    local_result = evaluate(\n",
    "        data=str(eval_data_path),\n",
    "        evaluators=evaluators,\n",
    "        evaluator_config=evaluator_config\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Local evaluation completed!\")\n",
    "    \n",
    "    # Display results\n",
    "    metrics = local_result['metrics']\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"ğŸ“Š {metric_name}: {value:.4f}\")\n",
    "        \n",
    "        # Save results locally\n",
    "        local_results_filename = os.environ.get(\"LOCAL_RESULTS_FILENAME\", \"local_evaluation_results.json\")\n",
    "        with open(local_results_filename, \"w\") as f:\n",
    "            json.dump(local_result, f, indent=2)\n",
    "\n",
    "        print(f\"ğŸ’¾ Results saved to: {local_results_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Local evaluation failed: {e}\")\n",
    "    local_result = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d525400",
   "metadata": {},
   "source": [
    "## â˜ï¸ Cloud Evaluation\n",
    "\n",
    "Upload evaluation results to Azure AI Foundry project for tracking and collaboration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "965f796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Evaluation - Following Official Microsoft Documentation\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import (\n",
    "    EvaluatorConfiguration,\n",
    "    EvaluatorIds,\n",
    "    Evaluation,\n",
    "    InputDataset\n",
    ")\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "print(\"â˜ï¸ Setting up Cloud Evaluation with Azure AI Foundry...\")\n",
    "\n",
    "# Configuration from environment variables\n",
    "AI_FOUNDRY_PROJECT_ENDPOINT = os.environ.get(\"AI_FOUNDRY_PROJECT_ENDPOINT\")\n",
    "TENANT_ID = os.environ.get(\"TENANT_ID\")\n",
    "\n",
    "print(f\"ğŸ¢ Foundry Project Endpoint: {AI_FOUNDRY_PROJECT_ENDPOINT}\")\n",
    "print(f\"ğŸ”‘ Tenant ID: {TENANT_ID}\")\n",
    "\n",
    "if not AI_FOUNDRY_PROJECT_ENDPOINT:\n",
    "    print(\"âš ï¸ Missing AI_FOUNDRY_PROJECT_ENDPOINT in .env file\")\n",
    "    cloud_result = None\n",
    "else:\n",
    "    try:\n",
    "        # Step 1: Create project client using DefaultAzureCredential (as per Microsoft docs)\n",
    "        print(\"ğŸ” Setting up authentication...\")\n",
    "        project_client = AIProjectClient(\n",
    "            endpoint=AI_FOUNDRY_PROJECT_ENDPOINT,\n",
    "            credential=DefaultAzureCredential(),\n",
    "        )\n",
    "        print(\"âœ… AIProjectClient created successfully!\")\n",
    "\n",
    "        # Step 2: Upload evaluation data to Azure AI Foundry (required for cloud evaluation)\n",
    "        print(\"ğŸ“¤ Uploading evaluation data to Azure AI Foundry...\")\n",
    "        dataset_name = os.environ.get(\"DATASET_NAME\", f\"health-fitness-dataset-{int(time.time())}\")\n",
    "        dataset_version = os.environ.get(\"DATASET_VERSION\", \"1.0\")\n",
    "        try:\n",
    "            data_upload = project_client.datasets.upload_file(\n",
    "                name=dataset_name,\n",
    "                version=dataset_version,\n",
    "                file_path=str(eval_data_path),\n",
    "            )\n",
    "            data_id = data_upload.id\n",
    "            print(f\"âœ… Data uploaded successfully! Dataset ID: {data_id}\")\n",
    "        except Exception as upload_error:\n",
    "            print(f\"âŒ Data upload failed: {upload_error}\")\n",
    "            raise upload_error\n",
    "\n",
    "        # Step 3: Configure evaluators using Azure AI Foundry built-in evaluators\n",
    "        print(\"âš™ï¸ Configuring evaluators for cloud evaluation...\")\n",
    "\n",
    "        evaluators = {\n",
    "            \"bleu_score\": EvaluatorConfiguration(\n",
    "                id=EvaluatorIds.BLEU_SCORE.value,\n",
    "                data_mapping={\n",
    "                    \"response\": \"${data.response}\",\n",
    "                    \"ground_truth\": \"${data.ground_truth}\",\n",
    "                },\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # Step 4: Create and submit evaluation\n",
    "        print(\"ğŸš€ Creating and submitting cloud evaluation...\")\n",
    "        evaluation_name = os.environ.get(\"EVALUATION_NAME\", f\"health-fitness-eval-{int(time.time())}\")\n",
    "        evaluation = Evaluation(\n",
    "            display_name=evaluation_name,\n",
    "            description=\"Health and fitness AI response evaluation\",\n",
    "            data=InputDataset(id=data_id),\n",
    "            evaluators=evaluators,\n",
    "        )\n",
    "\n",
    "        # Submit the evaluation\n",
    "        evaluation_response = project_client.evaluations.create(evaluation)\n",
    "\n",
    "        print(\"ğŸ‰ CLOUD EVALUATION SUBMITTED!\")\n",
    "        print(f\"   ğŸ“‹ Name: {evaluation_response.name}\")\n",
    "        print(f\"   ğŸ“‹ Status: {evaluation_response.status}\")\n",
    "        print(f\"   ğŸ“‹ Response Type: {type(evaluation_response)}\")\n",
    "\n",
    "        # Get evaluation ID - handle different possible attribute names\n",
    "        evaluation_id = None\n",
    "        if hasattr(evaluation_response, 'id'):\n",
    "            evaluation_id = evaluation_response.id\n",
    "        elif hasattr(evaluation_response, 'name'):\n",
    "            evaluation_id = evaluation_response.name  # Use name as ID if no separate ID exists\n",
    "\n",
    "        if evaluation_id:\n",
    "            print(f\"   ğŸ“‹ ID: {evaluation_id}\")\n",
    "\n",
    "        print(f\"\\nğŸ”— View detailed results at: https://ai.azure.com/\")\n",
    "        print(\"   Navigate to your project â†’ Evaluation â†’ View evaluation runs\")\n",
    "\n",
    "        # Save results\n",
    "        cloud_result = {\n",
    "            \"evaluation_name\": evaluation_response.name,\n",
    "            \"status\": evaluation_response.status,\n",
    "            \"project_endpoint\": AI_FOUNDRY_PROJECT_ENDPOINT,\n",
    "            \"dataset_id\": data_id,\n",
    "            \"timestamp\": int(time.time()),\n",
    "        }\n",
    "\n",
    "        # Add evaluation ID if available\n",
    "        if evaluation_id:\n",
    "            cloud_result[\"evaluation_id\"] = evaluation_id\n",
    "\n",
    "        with open(os.environ.get(\"CLOUD_RESULTS_FILENAME\", \"cloud_evaluation_results.json\"), \"w\") as f:\n",
    "            json.dump(cloud_result, f, indent=2, default=str)\n",
    "        print(f\"ğŸ’¾ Results saved to: {os.environ.get('CLOUD_RESULTS_FILENAME', 'cloud_evaluation_results.json')}\")\n",
    "\n",
    "        print(\"\\nâœ… SUCCESS: Cloud evaluation submitted to Azure AI Foundry!\")\n",
    "        print(\"   The evaluation will run in the cloud and results will be available in the Azure AI Foundry portal.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Cloud evaluation failed: {e}\")\n",
    "        print(f\"ğŸ“‹ Error type: {type(e).__name__}\")\n",
    "\n",
    "        # Enhanced error handling\n",
    "        error_str = str(e).lower()\n",
    "        if \"401\" in error_str or \"unauthorized\" in error_str:\n",
    "            print(\"\\nğŸ” AUTHENTICATION ISSUE:\")\n",
    "            print(\"   - Make sure you're logged in with: az login\")\n",
    "            print(\"   - Ensure you have access to the Azure AI Foundry project\")\n",
    "        elif \"403\" in error_str or \"forbidden\" in error_str:\n",
    "            print(\"\\nğŸš« PERMISSION ISSUE:\")\n",
    "            print(\"   - Verify you have 'AI Developer' or 'Contributor' role\")\n",
    "            print(\"   - Check Azure AI Foundry project permissions\")\n",
    "        elif \"404\" in error_str or \"not found\" in error_str:\n",
    "            print(\"\\nğŸ” RESOURCE NOT FOUND:\")\n",
    "            print(\"   - Verify AI_FOUNDRY_PROJECT_ENDPOINT is correct\")\n",
    "            print(\"   - Check if project exists in Azure AI Foundry\")\n",
    "        elif \"storage\" in error_str or \"blob\" in error_str:\n",
    "            print(\"\\nğŸ’¾ STORAGE ISSUE:\")\n",
    "            print(\"   - Ensure your Azure AI Foundry project has a connected storage account\")\n",
    "            print(\"   - Check storage account permissions for the project\")\n",
    "        else:\n",
    "            print(f\"\\nğŸ’¡ TROUBLESHOOTING:\")\n",
    "            print(f\"   - Full error: {str(e)[:300]}...\")\n",
    "            print(\"   - Try running local evaluation first\")\n",
    "            print(\"   - Check Azure AI Foundry project configuration\")\n",
    "\n",
    "        cloud_result = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3035d7d9",
   "metadata": {},
   "source": [
    "## ğŸ§¹ Cleanup Evaluation Files\n",
    "\n",
    "Remove all generated evaluation data and results files to keep the workspace clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d9fac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup evaluation files\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"ğŸ§¹ Cleaning up evaluation files...\")\n",
    "\n",
    "files_to_cleanup = [\n",
    "    os.environ.get(\"EVAL_DATA_FILENAME\", \"health_fitness_eval_data.jsonl\"),\n",
    "    os.environ.get(\"LOCAL_RESULTS_FILENAME\", \"local_evaluation_results.json\"),\n",
    "    os.environ.get(\"CLOUD_RESULTS_FILENAME\", \"cloud_evaluation_results.json\")\n",
    "]\n",
    "\n",
    "deleted_count = 0\n",
    "for file_path in files_to_cleanup:\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(f\"âœ… Deleted: {file_path}\")\n",
    "            deleted_count += 1\n",
    "        else:\n",
    "            print(f\"âš ï¸  File not found (already deleted?): {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error deleting {file_path}: {e}\")\n",
    "\n",
    "if deleted_count > 0:\n",
    "    print(f\"\\nğŸ‰ Cleanup completed! Deleted {deleted_count} file(s).\")\n",
    "else:\n",
    "    print(\"\\nâœ… No files to cleanup (all already deleted).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
