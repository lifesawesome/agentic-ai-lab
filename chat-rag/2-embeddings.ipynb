{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca3e8f66",
   "metadata": {},
   "source": [
    "# üèãÔ∏è Fun with Text and Image Embeddings üçé\n",
    "\n",
    "Welcome to our **Health & Fitness** embeddings notebook! In this tutorial, we'll show you how to:\n",
    "\n",
    "1. **Initialize** an `AIProjectClient` to access your Azure AI Foundry project.\n",
    "2. **Embed text** using `azure-ai-inference` with our fun health-themed phrases.\n",
    "3. **Embed images** using `azure-ai-inference` (we're using Cohere's image embeddings model).\n",
    "4. **Generate a health-themed image** (example code) and display it.\n",
    "5. **Use a prompt template** for extra context.\n",
    "\n",
    "Let's get started and have some fun with our healthy ideas! üçè\n",
    "\n",
    "> **Disclaimer**: This notebook is for educational purposes only. Always consult a professional for medical advice.\n",
    "\n",
    "<img src=\"./seq-diagrams/2-embeddings.png\" width=\"75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd2ac6",
   "metadata": {},
   "source": [
    "## 1. Setup & Environment\n",
    "\n",
    "#### Prerequisites:\n",
    "- Deploy a [text embeddings model](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/understand-embeddings) (**text-embedding-3-small**) in Azure AI Foundry\n",
    "- (Optional) Deploy a image embeddings model (**Cohere-embed-v3-english**) in Azure AI Foundry\n",
    "#\n",
    "We'll import our libraries and load the environment variables for:\n",
    "- `AI_FOUNDRY_PROJECT_ENDPOINT`: Your AI Foundry project endpoint.\n",
    "- `TEXT_EMBEDDING_MODEL`: The text embeddings model deployment name.\n",
    "- (Optional) `IMAGE_EMBEDDING_MODEL`: The image embeddings model deployment name.\n",
    "\n",
    "We'll import libraries, load environment variables, and create an `AIProjectClient`.\n",
    "\n",
    "> #### Complete [1-basic-chat-completion.ipynb](./1-basic-chat-completion.ipynb) notebook before starting this one\n",
    "Let's begin! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50899c96",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "from azure.identity import InteractiveBrowserCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.inference.models import ImageEmbeddingInput\n",
    "\n",
    "# Load environment variables\n",
    "notebook_path = Path().absolute()\n",
    "parent_dir = notebook_path.parent\n",
    "load_dotenv(parent_dir / '.env')\n",
    "\n",
    "# Retrieve from environment or fallback\n",
    "AI_FOUNDRY_PROJECT_ENDPOINT = os.environ.get(\n",
    "    \"AI_FOUNDRY_PROJECT_ENDPOINT\", \"<your-foundry-endpoint>\"\n",
    ")\n",
    "TEXT_EMBEDDING_MODEL = os.environ.get(\"EMBEDDING_MODEL_DEPLOYMENT_NAME\", \"text-embedding-3-small\")\n",
    "IMAGE_EMBEDDING_MODEL = os.environ.get(\"IMAGE_EMBEDDING_MODEL\", \"Cohere-embed-v3-english\")\n",
    "tenant_id = os.environ.get(\"TENANT_ID\")\n",
    "\n",
    "print(f\"üîë Using Tenant ID: {tenant_id}\")\n",
    "\n",
    "# Initialize project client with browser-based authentication (bypassing Azure CLI entirely)\n",
    "try:\n",
    "    print(\"üåê Using browser-based authentication to bypass Azure CLI cache issues...\")\n",
    "\n",
    "    # Use only InteractiveBrowserCredential with the specific tenant\n",
    "    credential = InteractiveBrowserCredential(tenant_id=tenant_id)\n",
    "\n",
    "    # Test the credential by getting a token for the correct tenant\n",
    "    print(\"üîÑ Testing credential...\")\n",
    "    test_token = credential.get_token(\"https://management.azure.com/.default\")\n",
    "    print(\"‚úÖ Successfully initialized Azure credentials with correct tenant!\")\n",
    "\n",
    "    # Create the project client using endpoint (connection_string is actually the endpoint URL)\n",
    "    project_client = AIProjectClient(\n",
    "        endpoint=AI_FOUNDRY_PROJECT_ENDPOINT, credential=credential\n",
    "    )\n",
    "    print(\"üéâ Successfully created AIProjectClient\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error creating AIProjectClient:\", e)\n",
    "    print(\"üí° Please complete the browser authentication prompt that should appear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e970dcce",
   "metadata": {},
   "source": [
    "## 2. Text Embeddings\n",
    "\n",
    "We'll call `get_embeddings_client()` from our `AIProjectClient` to retrieve the embeddings client. Then we'll embed some fun health-themed phrases:\n",
    "\n",
    "- \"üçé An apple a day keeps the doctor away\"\n",
    "- \"üèãÔ∏è 15-minute HIIT workout routine\"\n",
    "- \"üßò Mindful breathing exercises\"\n",
    "\n",
    "The output will be numeric vectors representing each phrase in semantic space. Let‚Äôs see those embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28466a95",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [],
   "source": [
    "text_phrases = [\n",
    "    \"An apple a day keeps the doctor away üçé\",\n",
    "    \"Quick 15-minute HIIT workout routine üèãÔ∏è\",\n",
    "    \"Mindful breathing exercises üßò\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    with project_client.get_openai_client(api_version=\"2024-10-21\") as embed_client:\n",
    "        response = embed_client.embeddings.create(\n",
    "            model=TEXT_EMBEDDING_MODEL,\n",
    "            input=text_phrases\n",
    "        )\n",
    "\n",
    "        for item in response.data:\n",
    "            vec = item.embedding\n",
    "            sample_str = f\"[{vec[0]:.4f}, {vec[1]:.4f}, ..., {vec[-2]:.4f}, {vec[-1]:.4f}]\"\n",
    "            print(f\"Sentence {item.index}: '{text_phrases[item.index]}':\\n\" \\\n",
    "                  f\"  Embedding length={len(vec)}\\n\" \\\n",
    "                  f\"  Sample: {sample_str}\\n\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error embedding text:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b0402",
   "metadata": {},
   "source": [
    "## 3. Prompt Template Example üìù\n",
    "\n",
    "Even though our focus is on embeddings, here's how you might prepend some context to a user message. Imagine you want to embed user text but first add a system prompt such as ‚ÄúYou are HealthFitGPT, a fitness guidance model‚Ä¶‚Äù This little extra helps set the stage for more context-aware embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f3e392",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [],
   "source": [
    "# A basic prompt template (system-style) we'll prepend to user text.\n",
    "TEMPLATE_SYSTEM = (\n",
    "    \"You are HealthFitGPT, a fitness guidance model.\\n\"\n",
    "    \"Please focus on healthy advice and disclaim you're not a doctor.\\n\\n\"\n",
    "    \"User message:\"  # We'll append the user message after this.\n",
    ")\n",
    "\n",
    "def embed_with_template(user_text):\n",
    "    \"\"\"Embed user text with a system template in front.\"\"\"\n",
    "    content = TEMPLATE_SYSTEM + \" \" + user_text\n",
    "    with project_client.get_openai_client(api_version=\"2024-10-21\") as embed_client:\n",
    "        rsp = embed_client.embeddings.create(\n",
    "            model=TEXT_EMBEDDING_MODEL,\n",
    "            input=[content]\n",
    "        )\n",
    "    return rsp.data[0].embedding\n",
    "\n",
    "sample_user_text = \"Can you suggest a quick home workout for busy moms?\"\n",
    "embedding_result = embed_with_template(sample_user_text)\n",
    "print(\"Embedding length:\", len(embedding_result))\n",
    "print(\"First few dims:\", embedding_result[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2571c972",
   "metadata": {},
   "source": [
    "## 4. Image Embeddings\n",
    "\n",
    "Image embeddings typically require managed compute resources. While Azure AI Foundry offers specialized models (like **MedImageInsight**) for medical images, in this example we'll use Cohere's serverless image embedding model.\n",
    "\n",
    "Here we are using the **`hand-xray.png`** image to generate embeddings. This image (of a hand X-ray) is our fun nod to health-themed imagery!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353bbc40",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # For image embeddings, we'll use a different approach since Cohere models\n",
    "    # are typically accessed through the inference client differently\n",
    "    print(\"üîç Image embeddings require specific model deployment and may not be\")\n",
    "    print(\"   available through the standard Azure OpenAI client.\")\n",
    "    print(\"   This section demonstrates the concept but may need model-specific configuration.\")\n",
    "    \n",
    "    # Placeholder for when proper image embedding support is available\n",
    "    print(\"‚ùå Image embedding functionality needs to be configured with the appropriate\")\n",
    "    print(\"   Cohere image embedding model deployment and client setup.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error embedding image:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb863ae9",
   "metadata": {},
   "source": [
    "## 5. Generate a Health-Related Image üèÉ (Optional)\n",
    "\n",
    "> **Note**: Image generation is currently not supported in the `azure-ai-inference` SDK. The example below uses the `openai` SDK with Azure OpenAI's DALL-E 3 model. This section will be updated once image generation is supported in the `azure-ai-inference` SDK.\n",
    "\n",
    "Let's generate a health-themed image using Azure OpenAI's DALL-E-3 model. You'll need:\n",
    "1. A DALL-E-3 model deployment in your Azure OpenAI resource\n",
    "2. The `openai` Python package installed (`pip install openai`)\n",
    "3. Your Azure OpenAI endpoint and API key set in environment variables\n",
    "\n",
    "We'll pass a simple prompt describing a healthy scenario and display the generated image inline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07b38d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import requests\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "def generate_health_image(prompt=\"A simple cartoon of a happy person jogging outdoors\"):\n",
    "    try:\n",
    "        # Initialize Azure OpenAI client\n",
    "        client = AzureOpenAI(\n",
    "            api_version=\"2024-02-01\",\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\", \"YOUR-API-KEY\"),\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"YOUR-AZURE-ENDPOINT\")\n",
    "        )\n",
    "        \n",
    "        # Generate image\n",
    "        result = client.images.generate(\n",
    "            model=\"dall-e-3\", # your DALL-E 3 deployment name\n",
    "            prompt=prompt,\n",
    "            n=1\n",
    "        )\n",
    "        \n",
    "        # Download and display image\n",
    "        image_url = result.data[0].url\n",
    "        image_data = requests.get(image_url).content\n",
    "        \n",
    "        # Save temporarily and display\n",
    "        with open(\"temp_image.png\", \"wb\") as f:\n",
    "            f.write(image_data)\n",
    "        img = Image.open(\"temp_image.png\")\n",
    "        display(img)\n",
    "        \n",
    "        # Clean up\n",
    "        os.remove(\"temp_image.png\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error generating image:\", e)\n",
    "\n",
    "# Let's try generating a health image\n",
    "generate_health_image(\"A watercolor painting of fresh fruits and vegetables arranged in a heart shape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c6dece",
   "metadata": {},
   "source": [
    "## 6. Wrap-Up & Next Steps\n",
    "üéâ We've shown how to:\n",
    "- Set up the `AIProjectClient`.\n",
    "- Get **text embeddings** using *text-embedding-3-small*.\n",
    "- Get **image embeddings** using *Cohere-embed-v3-english* on a health-themed (hand X-ray) image.\n",
    "- **Generate** a health-themed image (example code).\n",
    "- Use a **prompt template** to add system context to your embeddings.\n",
    "\n",
    "**Where to go next?**\n",
    "- Explore `azure-ai-evaluation` for evaluating your embeddings.\n",
    "- Use `azure-core-tracing-opentelemetry` for end-to-end telemetry.\n",
    "- Build out a retrieval pipeline to compare similarity of embeddings.\n",
    "\n",
    "Have fun experimenting, and remember: when it comes to your health, always consult a professional!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
